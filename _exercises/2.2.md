---
week: 2
lab: 2
lang: python
title: Manipulating data
description: In this set of exercises we will take some data from the internet, manipulate it and generate files on disk.
---

{% assign challenges = site.challenges | where:"lab", 2.2 | sort: "id" %}

In this set of exercises we will develop some structure in our code using [functions]({{"/references/functions" | relative_url}}) and start to work with more complex data.
The dataset we will be using comes from the [star wars API](https://swapi.py4e.com/) which provides access to json-formatted data for the well-known star wars films.

We will access the data via HTTP requests using the `urllib` module.
We will also review interpreting json formatted data and storing them in files.
Finally, we will create some more complex logic to pull lots of data and extract specific details from them.

{% include toc.md %}


## Functions

Think of functions as recipes.
We create functions as reusable bits of code.
We often need to do the same things multiple times.
Having a recipe avoids repetition.

```python
def html_tag(content, tag):
    return f"<{tag}>{content}</{tag}>"

heading = html_tag('hello world', 'h1') 
subheading = html_tag('reusable functions', 'h2')
```

> The above example is simple, it adds HTML tags around some content.

As we solve problems, our code becomes more complex.
It can be useful to package up the details into suitably-named higher-level functions and move them out of the way, perhaps into another module (file).
This helps us to keep our code tidy and focus on what we are trying to do.

```python
def html_tag(content, tag, **attributes):
    attrs = [f'{k}="{v}"' for k, v in attributes.items()]
    attrs = " ".join(attrs)
    if attrs:
        attrs = " " + attrs
    return f"<{tag}{attrs}>{content}</{tag}>"
```

> Now we have some processing for HTML attributes.

When we organise our code like this, we should pay close attention to the names we give our functions.
These names become embedded in our programmes.
when we read our code it should be obvious what we are doing.

```python
p = html_tag(
    "this is a reusable function", "p", 
    id="convenient", style="color: red"
)
```

{{ challenges[0] }}


## Making HTTP requests with `urllib`

To make HTTP requests we can import functions from the `urllib` package, which is part of the python standard library.

> The standard library includes vast amounts of useful python code maintained by the core python developers.
> We will only scratch the surface of the standard library.
> We've already looked at `json` and `pathlib`, now we are looking briefly at the `urllib.request` module.

Making simple HTTP requests is pretty easy.
We can just import the `urlopen` function from the `urllib.request` module and call it with a url as an argument.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
```

In the above code, we are requesting the HTML file containing the previous exercise.
The request generates an `http.client.HTTPResponse` object.

> This is an instance of the built in `HTTPResponse` class which can be found in the `client` module of the `http` package.

To access the data, we need to call the `HTTPResponse.read()` method and do some processing.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data[:95])
```

Calling `read` on the response object extracts the body of the response as a sequence of bytes. We convert this into a string using utf-8 encoding so we do this on one line.

> As you may notice, `urllib` requires a bit of work to use it. 
The [urllib3](https://pypi.org/project/urllib3/) and [requests](https://pypi.org/project/requests/) libraries are popular additions which provide more power and a cleaner API.

Finally, we print a small chunk of the file.
The output looks like this (at the time of writing).

```plaintext
<!DOCTYPE html>
<html lang="en">
<head>
<title>Files and folders</title>
<meta charset="utf-8">
```

So, with a small amount of work, we can access data from anywhere on the public web.

{{challenges[1]}}

>If you wanted to parse an HTML file (e.g. for scraping data from a website), then a popular library is [beautifulsoup](https://pypi.org/project/beautifulsoup4/).

## JSON APIs

*Web services* that provide JSON formatted data over HTTP are increasingly popular.
The [star wars API](https://swapi.py4e.com/) is a toy example of this.
It provides json files containing information about the star wars films.

> It's pretty out of date, but we will use it anyway.

We can get data by making HTTP requests to [https://swapi.py4e.com/api/](https://swapi.py4e.com/api/) and other *endpoints* which are specified in [the documentation](https://swapi.py4e.com/documentation).

```python
from urllib.request import urlopen

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data)
```

The output is a json-formatted string with the keys "people", "planets", "films", "species", "vehicles" and "starships".
Each key has a url to the appropriate document as the value.

```json
{"people":"https://swapi.py4e.com/api/people/","planets":"https://swapi.py4e.com/api/planets/","films":"https://swapi.py4e.com/api/films/","species":"https://swapi.py4e.com/api/species/","vehicles":"https://swapi.py4e.com/api/vehicles/","starships":"https://swapi.py4e.com/api/starships/"}
```

To extract the data into a python dictionary, we need to pass the resultant string into the `json.loads()` function. 
We can update our script as follows:

```python
from urllib.request import urlopen
import json

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = json.loads(response.read().decode('utf-8'))

for key, value in data.items(): 
    print(f"{key}: {value}")
```

We loop over the dictionary items and print each in turn.

```plaintext
people: https://swapi.py4e.com/api/people/
planets: https://swapi.py4e.com/api/planets/
films: https://swapi.py4e.com/api/films/
species: https://swapi.py4e.com/api/species/
vehicles: https://swapi.py4e.com/api/vehicles/
starships: https://swapi.py4e.com/api/starships/
```

We can see that the API has given us more urls to investigate.
So we can add something like this to the end of our script:

```python
response = urlopen(data['people'])
people = json.loads(response.read().decode('utf-8'))
```

Now in two lines of code we have grabbed more data from the web.

Study the result.
You should see that the dictionary keys include `'count'`, `'next'`, `'previous'` and `'results'`. 
The API provides the data in pages, so we have only grabbed the first page of data with this request.
The url in the `'next'` key will grab the next page.
The `'results'` key contains the actual data.

> Different API's will have different formats, when writing an application that interacts with an API, you usually need to encapsulate the specifics of the data structures into your code.

We can perform a [list comprehension]({{"references/list-comprehensions" | relative_url }}) to extract a list of names from the first page.

```python
[p['name'] for p in people['results']]
```

The result is a simple list.

```plaintext
['Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi']
```

{{ challenges[2] }}

## Manipulating data in more complex ways

Now we have a large dataset at our fingertips we can write programmes and functions to answer simple questions.

