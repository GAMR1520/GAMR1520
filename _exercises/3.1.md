---
week: 3
lab: 1
lang: python
title: Accessing data via HTTP
description: This exercise introduces data requested from the web. We interact with a JSON API and look at manipulating complex data.
---

{% assign challenges = site.challenges | where:"lab", 3.2 | sort: "id" %}

In this set of exercises we will load data from the web.
Beginning with a simple HTML example we will see how easy it is to load data into python.

We will then move on to using a JSON API.
The dataset we will be using comes from a star wars API ([swapi](https://swapi.py4e.com/)) which provides access to json-formatted data relating to the star wars films.

We will develop some moderately complex code to access the data via HTTP requests, interpret them as json formatted data and store them in files.
Finally, we will write functions to extract specific details from the data and generate some outputs.

## Making HTTP requests with `urllib`

- We need some data to play with. 
- The web has lot's of data.
- Let's get data over HTTP.

To make HTTP requests we can import functions from the `urllib` package, which is part of the python standard library.

> The standard library includes vast amounts of useful python code maintained by the core python developers.
> We will only scratch the surface of the standard library.
> We've already looked at `json` and `pathlib`, now we are looking briefly at the `urllib.request` module.

Making simple HTTP requests is pretty easy.
We can just import the `urlopen` function from the `urllib.request` module and call it with a url as an argument.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
```

In the above code, we are requesting the HTML file containing the previous exercise.
The request generates an `http.client.HTTPResponse` object.

> Our `response` variable is assigned to an instance of the built in `HTTPResponse` class which can be found in the `client` module of the `http` package.

To access the content of the response, we need to call the `HTTPResponse.read()` method.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data[:95])
```

Calling `read` on the response object extracts the body of the response as a sequence of bytes. Notice we are converting this into a string using *utf-8* encoding. 
We do this on one line because we can.

> As you may notice, `urllib` requires a bit of work to use it. 
The [urllib3](https://pypi.org/project/urllib3/) and [requests](https://pypi.org/project/requests/) libraries are popular additions which provide more power and a cleaner API.

Finally, we print a small chunk of the file.
The output looks like this (at the time of writing).

```plaintext
<!DOCTYPE html>
<html lang="en">
<head>
<title>Files and folders</title>
<meta charset="utf-8">
```

So, with a small amount of work, we can access data from anywhere on the public web.

{{challenges[0]}}

>If you wanted to parse an HTML file (e.g. for scraping data from a website), then a popular library is [beautifulsoup](https://pypi.org/project/beautifulsoup4/).

## JSON APIs

*Web services* that provide JSON formatted data over HTTP are increasingly popular.
The [star wars API](https://swapi.py4e.com/) is a toy example of this.
It provides json files containing information about the star wars films.

> It's pretty out of date, but we will use it anyway.

We can get data by making HTTP requests to [https://swapi.py4e.com/api/](https://swapi.py4e.com/api/) and other *endpoints* which are specified in [the documentation](https://swapi.py4e.com/documentation).

```python
from urllib.request import urlopen

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data)
```

The output is a json-formatted string with the keys "people", "planets", "films", "species", "vehicles" and "starships".
Each key has a url to the appropriate document as the value.

```json
{"people":"https://swapi.py4e.com/api/people/","planets":"https://swapi.py4e.com/api/planets/","films":"https://swapi.py4e.com/api/films/","species":"https://swapi.py4e.com/api/species/","vehicles":"https://swapi.py4e.com/api/vehicles/","starships":"https://swapi.py4e.com/api/starships/"}
```

To extract the data into a python dictionary, we need to pass the resultant string into the `json.loads()` function. 
We can update our script as follows:

```python
from urllib.request import urlopen
import json

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = json.loads(response.read().decode('utf-8'))
```

We can loop over the dictionary items and print each in turn.

```python
for key, value in data.items(): 
    print(f"{key}: {value}")
```

```plaintext
people: https://swapi.py4e.com/api/people/
planets: https://swapi.py4e.com/api/planets/
films: https://swapi.py4e.com/api/films/
species: https://swapi.py4e.com/api/species/
vehicles: https://swapi.py4e.com/api/vehicles/
starships: https://swapi.py4e.com/api/starships/
```

We can see that the API has given us more urls to investigate.
So we can add something like this to the end of our script:

```python
response = urlopen(data['people'])
people = json.loads(response.read().decode('utf-8'))
```

Now in two lines of code we have grabbed more data from the web.

{{ challenges[1] }}

## Paged data

The API provides the data in pages.
Study the `people` data.
You should see that the dictionary keys include `'count'`, `'next'`, `'previous'` and `'results'`.
We have only grabbed the first page of data with this request.
The `'results'` key contains the data for this page.
The url in the `'next'` key will grab the next page.

> Different API's will have different formats, when writing an application that interacts with an API, you usually need to encapsulate the specifics of the data structures into your code.

We can perform a [list comprehension]({{"references/list-comprehensions" | relative_url }}) to extract a list of names from the first page.

```python
[p['name'] for p in people['results']]
```

The result is a simple list.

```plaintext
['Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi']
```

However, this is only one page.
So we need to loop over all the pages and extract the necessary data.

You should already have a function which looks something like this.

```python
def load_json(url):
    """Load json data from a given url"""
    print(f"Requesting: {url}")
    response = urlopen(url)
    json_data = response.read().decode('utf-8')
    return json.loads(json_data)
```

> If yours is better, let me know.
> I've added a print statement to indicate when we are requesting data from the web. 

{{ challenges[2] }}


To implement this, we can add another function to loop over the collection, one page at a time.

```python
def load_collection(url):
    result = []
    while True:
        chunk = load_json(url)
        result += chunk['results']
        if not chunk['next']: 
            break
        url = chunk['next']
    return result
```

See how the above function prepares a `result` list to contain the data it gathers.
It defines an infinite loop in which it loads the `chunk` of data, adds `chunk['results']` to the growing `result` list and then checks for a `'next'` key.
If it doesn't find a `'next'` key then it calls `break` to exit the loop and returns the result.
If there is a `'next'` key then it updates the `url` and repeats the loop to get the next page of data.

With the above two functions `load_json` and `load_collection` we can now do a lot with just a few lines of code.

```python
url = 'https://swapi.py4e.com/api/'
swapi = load_json(url)
people = load_collection(swapi['people'])
print([p['name'] for p in people])
```
{: .small-margin}
```plaintext
Requesting: https://swapi.py4e.com/api/
Requesting: https://swapi.py4e.com/api/people/
Requesting: https://swapi.py4e.com/api/people/?page=2
Requesting: https://swapi.py4e.com/api/people/?page=3
Requesting: https://swapi.py4e.com/api/people/?page=4
Requesting: https://swapi.py4e.com/api/people/?page=5
Requesting: https://swapi.py4e.com/api/people/?page=6
Requesting: https://swapi.py4e.com/api/people/?page=7
Requesting: https://swapi.py4e.com/api/people/?page=8
Requesting: https://swapi.py4e.com/api/people/?page=9
['Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi', 'Anakin Skywalker', 'Wilhuff Tarkin', 'Chewbacca', 'Han Solo', 'Greedo', 'Jabba Desilijic Tiure', 'Wedge Antilles', 'Jek Tono Porkins', 'Yoda', 'Palpatine', 'Boba Fett', 'IG-88', 'Bossk', 'Lando Calrissian', 'Lobot', 'Ackbar', 'Mon Mothma', 'Arvel Crynyd', 'Wicket Systri Warrick', 'Nien Nunb', 'Qui-Gon Jinn', 'Nute Gunray', 'Finis Valorum', 'Padmé Amidala', 'Jar Jar Binks', 'Roos Tarpals', 'Rugor Nass', 'Ric Olié', 'Watto', 'Sebulba', 'Quarsh Panaka', 'Shmi Skywalker', 'Darth Maul', 'Bib Fortuna', 'Ayla Secura', 'Ratts Tyerel', 'Dud Bolt', 'Gasgano', 'Ben Quadinaros', 'Mace Windu', 'Ki-Adi-Mundi', 'Kit Fisto', 'Eeth Koth', 'Adi Gallia', 'Saesee Tiin', 'Yarael Poof', 'Plo Koon', 'Mas Amedda', 'Gregar Typho', 'Cordé', 'Cliegg Lars', 'Poggle the Lesser', 'Luminara Unduli', 'Barriss Offee', 'Dormé', 'Dooku', 'Bail Prestor Organa', 'Jango Fett', 'Zam Wesell', 'Dexter Jettster', 'Lama Su', 'Taun We', 'Jocasta Nu', 'R4-P17', 'Wat Tambor', 'San Hill', 'Shaak Ti', 'Grievous', 'Tarfful', 'Raymus Antilles', 'Sly Moore', 'Tion Medon', 'Finn', 'Rey', 'Poe Dameron', 'BB8', 'Captain Phasma']
```
{: .small-margin}

However, every time we call the function we have to wait for the API calls to complete.
We could refactor the code to make the requests asynchronous and in parallel, but this is outside of the scope of this module.
Instead, we will cache the results in files so that subsequent requests can get the local data rather than making a request to the API.

So, we want to define a new function which will check to see if we have a local copy of the data from a given url.

For this we need to define a mapping between the url and the filesystem path.
For example, the file we get from this url:

```plaintext
https://swapi.py4e.com/api/people/?page=4
```
Might be saved somewhere like this:

```plaintext
./swapi/people?page=4.json
```

So, we can define a simple function to convert a url to a pathlib.Path object.

```python
def path_from_url(url):
    root = Path('swapi')
    filename = f"{url[27:].strip('/').replace('/?', '_')}.json"
    return root / filename
```

Our function strips off the first 23 characters of the url as well as the trailing forward slash and it replaces the `'/?'` with a simple `'_'` to sanitise the filename a bit.
It also adds everything into a 'swapi' folder to hold all the downloaded files.

Now we can define a new `load_json_with_cache` function that will only make the HTTP request if we don't already have a copy.
If it does download some data, it will save it to a file before returning it.
If it finds a file, then it reads the local copy of the data and doesn't need to download anything.

```python
def load_json_with_cache(url):
    path = path_from_url(url)
    if not path.exists():
        data = load_json(url)
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open('w') as f:
            json.dump(data, f, indent=2)
        return data
    print(f"Opening file: {path}")
    with path.open('r') as f:
        return json.load(f)
```

With this, we can now update our `load_collection` function to use `load_json_with_cache` rather than the simpler `load_json` function.

```python
def load_collection(url):
    result = []
    while True:
        chunk = load_json_with_cache(url)
        result += chunk['results']
        if not chunk['next']: 
            break
        url = chunk['next']
    return result
```

Try loading the people data again and you should see a collection of files appear.

```python
url = 'https://swapi.py4e.com/api/'
swapi = load_json(url)
people = load_collection(swapi['people'])
print([p['name'] for p in people])
```
{: .small-margin}
```plaintext
Requesting: https://swapi.py4e.com/api/
Requesting: https://swapi.py4e.com/api/people/
Requesting: https://swapi.py4e.com/api/people/?page=2
Requesting: https://swapi.py4e.com/api/people/?page=3
Requesting: https://swapi.py4e.com/api/people/?page=4
Requesting: https://swapi.py4e.com/api/people/?page=5
Requesting: https://swapi.py4e.com/api/people/?page=6
Requesting: https://swapi.py4e.com/api/people/?page=7
Requesting: https://swapi.py4e.com/api/people/?page=8
Requesting: https://swapi.py4e.com/api/people/?page=9
['Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi', 'Anakin Skywalker', 'Wilhuff Tarkin', 'Chewbacca', 'Han Solo', 'Greedo', 'Jabba Desilijic Tiure', 'Wedge Antilles', 'Jek Tono Porkins', 'Yoda', 'Palpatine', 'Boba Fett', 'IG-88', 'Bossk', 'Lando Calrissian', 'Lobot', 'Ackbar', 'Mon Mothma', 'Arvel Crynyd', 'Wicket Systri Warrick', 'Nien Nunb', 'Qui-Gon Jinn', 'Nute Gunray', 'Finis Valorum', 'Padmé Amidala', 'Jar Jar Binks', 'Roos Tarpals', 'Rugor Nass', 'Ric Olié', 'Watto', 'Sebulba', 'Quarsh Panaka', 'Shmi Skywalker', 'Darth Maul', 'Bib Fortuna', 'Ayla Secura', 'Ratts Tyerel', 'Dud Bolt', 'Gasgano', 'Ben Quadinaros', 'Mace Windu', 'Ki-Adi-Mundi', 'Kit Fisto', 'Eeth Koth', 'Adi Gallia', 'Saesee Tiin', 'Yarael Poof', 'Plo Koon', 'Mas Amedda', 'Gregar Typho', 'Cordé', 'Cliegg Lars', 'Poggle the Lesser', 'Luminara Unduli', 'Barriss Offee', 'Dormé', 'Dooku', 'Bail Prestor Organa', 'Jango Fett', 'Zam Wesell', 'Dexter Jettster', 'Lama Su', 'Taun We', 'Jocasta Nu', 'R4-P17', 'Wat Tambor', 'San Hill', 'Shaak Ti', 'Grievous', 'Tarfful', 'Raymus Antilles', 'Sly Moore', 'Tion Medon', 'Finn', 'Rey', 'Poe Dameron', 'BB8', 'Captain Phasma']
```
{: .small-margin}

Now the files have been saved, if you run the function again it won't be delayed by waiting for the API to respond because it will use the local files.

This is a general purpose (ish) solution and can be used for all of the *swapi* api.

{{ challenges[3] }}