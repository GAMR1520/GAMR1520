---
week: 3
lab: 2
lang: python
title: Accessing data via HTTP
description: This exercise introduces data requested from the web. We interact with a JSON API and look at manipulating complex data.
---

{% assign challenges = site.challenges | where:"lab", 2.3 | sort: "id" %}

In this set of exercises we will load data from the web.
Beginning with a simple HTML example we will see how easy it is to load data into python.

We will then move on to using a JSON API.
The dataset we will be using comes from a star wars API ([swapi](https://swapi.py4e.com/)) which provides access to json-formatted data relating to the star wars films.

We will develop some moderately complex code to access the data via HTTP requests, interpret them as json formatted data and store them in files.
Finally, we will write functions to extract specific details from the data and generate some outputs.

## Making HTTP requests with `urllib`

- We need some data to play with. 
- The web has lot's of data.
- Let's get data over HTTP.

To make HTTP requests we can import functions from the `urllib` package, which is part of the python standard library.

> The standard library includes vast amounts of useful python code maintained by the core python developers.
> We will only scratch the surface of the standard library.
> We've already looked at `json` and `pathlib`, now we are looking briefly at the `urllib.request` module.

Making simple HTTP requests is pretty easy.
We can just import the `urlopen` function from the `urllib.request` module and call it with a url as an argument.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
```

In the above code, we are requesting the HTML file containing the previous exercise.
The request generates an `http.client.HTTPResponse` object.

> Our `response` variable is assigned to an instance of the built in `HTTPResponse` class which can be found in the `client` module of the `http` package.

To access the content of the response, we need to call the `HTTPResponse.read()` method.

```python
from urllib.request import urlopen

url = 'http://gamr1520.github.io/GAMR1520/exercises/2.1.html'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data[:95])
```

Calling `read` on the response object extracts the body of the response as a sequence of bytes. Notice we are converting this into a string using *utf-8* encoding. 
We do this on one line because we can.

> As you may notice, `urllib` requires a bit of work to use it. 
The [urllib3](https://pypi.org/project/urllib3/) and [requests](https://pypi.org/project/requests/) libraries are popular additions which provide more power and a cleaner API.

Finally, we print a small chunk of the file.
The output looks like this (at the time of writing).

```plaintext
<!DOCTYPE html>
<html lang="en">
<head>
<title>Files and folders</title>
<meta charset="utf-8">
```

So, with a small amount of work, we can access data from anywhere on the public web.

{{challenges[2]}}

>If you wanted to parse an HTML file (e.g. for scraping data from a website), then a popular library is [beautifulsoup](https://pypi.org/project/beautifulsoup4/).

## JSON APIs

*Web services* that provide JSON formatted data over HTTP are increasingly popular.
The [star wars API](https://swapi.py4e.com/) is a toy example of this.
It provides json files containing information about the star wars films.

> It's pretty out of date, but we will use it anyway.

We can get data by making HTTP requests to [https://swapi.py4e.com/api/](https://swapi.py4e.com/api/) and other *endpoints* which are specified in [the documentation](https://swapi.py4e.com/documentation).

```python
from urllib.request import urlopen

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = response.read().decode('utf-8')

print(data)
```

The output is a json-formatted string with the keys "people", "planets", "films", "species", "vehicles" and "starships".
Each key has a url to the appropriate document as the value.

```json
{"people":"https://swapi.py4e.com/api/people/","planets":"https://swapi.py4e.com/api/planets/","films":"https://swapi.py4e.com/api/films/","species":"https://swapi.py4e.com/api/species/","vehicles":"https://swapi.py4e.com/api/vehicles/","starships":"https://swapi.py4e.com/api/starships/"}
```

To extract the data into a python dictionary, we need to pass the resultant string into the `json.loads()` function. 
We can update our script as follows:

```python
from urllib.request import urlopen
import json

url = 'https://swapi.py4e.com/api/'
response = urlopen(url)
data = json.loads(response.read().decode('utf-8'))
```

We can loop over the dictionary items and print each in turn.

```python
for key, value in data.items(): 
    print(f"{key}: {value}")
```

```plaintext
people: https://swapi.py4e.com/api/people/
planets: https://swapi.py4e.com/api/planets/
films: https://swapi.py4e.com/api/films/
species: https://swapi.py4e.com/api/species/
vehicles: https://swapi.py4e.com/api/vehicles/
starships: https://swapi.py4e.com/api/starships/
```

We can see that the API has given us more urls to investigate.
So we can add something like this to the end of our script:

```python
response = urlopen(data['people'])
people = json.loads(response.read().decode('utf-8'))
```

Now in two lines of code we have grabbed more data from the web.

{{ challenges[3] }}

## Paged data

The API provides the data in pages.
Study the `people` data.
You should see that the dictionary keys include `'count'`, `'next'`, `'previous'` and `'results'`.
We have only grabbed the first page of data with this request.
The `'results'` key contains the data for this page.
The url in the `'next'` key will grab the next page.

> Different API's will have different formats, when writing an application that interacts with an API, you usually need to encapsulate the specifics of the data structures into your code.

We can perform a [list comprehension]({{"references/list-comprehensions" | relative_url }}) to extract a list of names from the first page.

```python
[p['name'] for p in people['results']]
```

The result is a simple list.

```plaintext
['Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi']
```

{{ challenges[4] }}

## Manipulating data in more complex ways

Now we have a large dataset at our fingertips we can write programmes and functions to answer simple questions.

